{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47de1bb7-4c93-4bb7-9e7a-8bb0f0ec2b5d",
   "metadata": {},
   "source": [
    "This is an implementation of the GPT tokenizer following [this video](https://www.youtube.com/watch?v=zduSFxRajkE) by Andrej Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "73a3ba73-adb1-4ce9-b9cb-0c4dfe0b6757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(ids, bigram: dict):\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        # pair = list(pair) # This might be preferable because of all the 'list(pair)' in the decoder\n",
    "        bigram[pair] = bigram.get(pair, 0) + 1\n",
    "    return bigram\n",
    "\n",
    "def merge(ids, new_pair, new_code):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids) - 1:\n",
    "        if (ids[i], ids[i + 1]) == new_pair:\n",
    "            new_ids.append(new_code)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    if i == len(ids) - 1:\n",
    "        new_ids.append(ids[-1])\n",
    "    return new_ids\n",
    "class BasicTokenizer:\n",
    "    \"\"\"\n",
    "    This is a simple implementation of a BPE tokenizer.\n",
    "    After initializing it you have to train it using .train\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        \"\"\"Train the BPE algorithm on text, by performing merges until the vocab size == vocab_size\"\"\"\n",
    "        assert vocab_size >= 256, \"The vocab_size has to be greater than 256\"\n",
    "        self.merges = {} # A history of all the merges that happend\n",
    "        ids = [x for x in text.encode('utf-8')]\n",
    "        len_ids_before = len(ids)\n",
    "        curr_vocab_size = 256\n",
    "        self.decode_map = {i: bytes([i]) for i in range(256)}\n",
    "        while curr_vocab_size < vocab_size and len(ids) > 1:\n",
    "            bigram = {}\n",
    "            get_bigram(ids, bigram)\n",
    "            new_pair = max(bigram, key=bigram.get)\n",
    "            if verbose:\n",
    "                print(f'Merging {new_pair} into id {curr_vocab_size}')\n",
    "            self.decode_map[curr_vocab_size] = self.decode_map[new_pair[0]] + self.decode_map[new_pair[1]]\n",
    "            ids = merge(ids, new_pair, curr_vocab_size)\n",
    "            self.merges[new_pair] = curr_vocab_size\n",
    "            curr_vocab_size += 1\n",
    "        self.vocab_size = curr_vocab_size\n",
    "        len_ids_after = len(ids)\n",
    "        if verbose:\n",
    "            print(f'The compression rate is {len_ids_before/len_ids_after:.2f}')\n",
    "\n",
    "    def encode(self, text):\n",
    "        # I've implemented this by myself. There are probably other algorithms that do the same things much faster\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        prev_tokens_len = 0\n",
    "        curr_tokens = tokens\n",
    "        curr_tokens_len = len(tokens)\n",
    "        while prev_tokens_len != curr_tokens_len:\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < curr_tokens_len - 1:\n",
    "                if (curr_tokens[i], curr_tokens[i + 1]) in self.merges:\n",
    "                    new_tokens.append(self.merges[(curr_tokens[i], curr_tokens[i + 1])])# This will help us decode\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(curr_tokens[i])\n",
    "                    i += 1\n",
    "            if i == curr_tokens_len - 1:\n",
    "                new_tokens.append(curr_tokens[-1])\n",
    "\n",
    "            prev_tokens_len = curr_tokens_len\n",
    "            curr_tokens = new_tokens\n",
    "            curr_tokens_len = len(new_tokens)\n",
    "        return curr_tokens\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        decoded_ids = b''.join([self.decode_map[idx] for idx in ids])\n",
    "        text = decoded_ids.decode('utf-8', errors='replace')\n",
    "        return text\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8a84a32b-9f21-47cb-8e3a-6bb18f53432b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mBasicTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Train the BPE algorithm on text, by performing merges until the vocab size == vocab_size\n",
       "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_193194/1854094153.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BasicTokenizer.train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f8a045f-7f3d-461c-9bd0-7f7dc4f81dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BasicTokenizer()\n",
    "taylor = open('data/taylorswift.txt' ,'r').read()\n",
    "small_test = 'Hello world this is a test!\\n'\n",
    "tok.train(taylor, 500, verbose=False)\n",
    "# Basic sanity check tests\n",
    "assert tok.decode(tok.encode(small_test)) == small_test\n",
    "tok = BasicTokenizer()\n",
    "tok.train(small_test, 500)\n",
    "assert tok.decode(tok.encode(taylor)) == taylor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d9a67bb-a966-4431-b075-89a8292181ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f82aaa3c-26d0-4b02-8830-158a429d2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexTokenizer:\n",
    "    \"\"\"A BPE tokenizer that splits the training/encoding text, before preforming training/tokenizing using BPE\"\"\" \n",
    "    def __init__(self, pattern=GPT4_SPLIT_PATTERN):\n",
    "        super().__init__()\n",
    "        self.pattern = re.compile(pattern)\n",
    "        \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256, \"The vocab_size has to be greater than 256\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = {} # A history of all the merges that happend\n",
    "        list_ids = [list(t.encode('utf-8')) for t in re.findall(self.pattern, text)]\n",
    "        len_ids_before = len(text.encode('utf-8')) # To calculate the compression ration\n",
    "        curr_vocab_size = 256\n",
    "        self.decode_map = {i: bytes([i]) for i in range(256)}\n",
    "        while curr_vocab_size < vocab_size:\n",
    "            bigram = {}\n",
    "            for ids in list_ids:\n",
    "                get_bigram(ids, bigram)\n",
    "            if bigram == {}: # 1 code per ids\n",
    "                break\n",
    "            new_pair = max(bigram, key=bigram.get)\n",
    "            if verbose:\n",
    "                print(f'Merging {new_pair} into id {curr_vocab_size}')\n",
    "            self.decode_map[curr_vocab_size] = self.decode_map[new_pair[0]] + self.decode_map[new_pair[1]] # This will help us decode\n",
    "            list_ids = [merge(ids, new_pair, curr_vocab_size) for ids in list_ids]\n",
    "            self.merges[new_pair] = curr_vocab_size\n",
    "            curr_vocab_size += 1\n",
    "        self.vocab_size = curr_vocab_size\n",
    "        len_ids_after = sum(len(ids) for ids in list_ids)\n",
    "        if verbose:\n",
    "            print(f'The compression rate is {len_ids_before/len_ids_after:.2f}')\n",
    "    def encode(self, text):\n",
    "        # I've implemented this by myself. There are probably other algorithms that do the same things much faster\n",
    "        list_tokens = [list(tokens.encode('utf-8')) for tokens in re.findall(self.pattern, text)]\n",
    "        prev_tokens_len = 0\n",
    "        curr_list_tokens = list_tokens\n",
    "        curr_tokens_len = len(text.encode('utf-8'))\n",
    "        while prev_tokens_len != curr_tokens_len:\n",
    "            prev_tokens_len = curr_tokens_len\n",
    "            curr_token_len = 0\n",
    "            new_list_tokens = []\n",
    "            for curr_tokens in curr_list_tokens:\n",
    "                i = 0\n",
    "                new_tokens = []\n",
    "                tokens_len = len(curr_tokens)\n",
    "                while i < tokens_len - 1:\n",
    "                    if (curr_tokens[i], curr_tokens[i + 1]) in self.merges:\n",
    "                        new_tokens.append(self.merges[(curr_tokens[i], curr_tokens[i + 1])])# This will help us decode\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(curr_tokens[i])\n",
    "                        i += 1\n",
    "                    curr_token_len += 1\n",
    "                if i == tokens_len - 1:\n",
    "                    new_tokens.append(curr_tokens[-1])\n",
    "                    curr_token_len += 1\n",
    "                new_list_tokens.append(new_tokens)\n",
    "            curr_list_tokens = new_list_tokens\n",
    "        encoded_tokens = [token for tokens in curr_list_tokens for token in tokens]\n",
    "        # print(f'The number of partitions is {len(curr_list_tokens)}, and they all have {sum(len(tokens) for tokens in curr_list_tokens)}')\n",
    "        return encoded_tokens\n",
    "    def decode(self, ids):\n",
    "        decoded_ids = b''.join([self.decode_map[idx] for idx in ids])\n",
    "        text = decoded_ids.decode('utf-8', errors='replace')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0cb7aee4-dc37-4bc4-9fb1-432a8df4c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = RegexTokenizer()\n",
    "taylor = open('data/taylorswift.txt' ,'r').read()\n",
    "small_test = 'Hello world this is a test!\\n'\n",
    "tok.train(small_test, 500, verbose=False)\n",
    "assert taylor == tok.decode(tok.encode(taylor))\n",
    "tok = RegexTokenizer()\n",
    "tok.train(taylor, 500, verbose=False)\n",
    "assert small_test == tok.decode(tok.encode(small_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f38f75-5a8e-476c-9d15-d230e8f9b871",
   "metadata": {},
   "source": [
    "After finishing this I have to move it to it's own .py file\\(s\\).\n",
    "\n",
    "This was made mostly for educational purposes because running a transformer on tokens is impossible for me due to compute limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080285f2-2117-4f85-b320-4d52a75fa8cb",
   "metadata": {},
   "source": [
    "I might expand this in the future to include more impelentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7f78a-8c71-4984-b688-aa2cee7485a0",
   "metadata": {},
   "source": [
    "Andrej Karpathy's [Implementation](https://github.com/karpathy/minbpe/blob/master/minbpe/gpt4.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
